{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JEX_W_NvL9r",
        "outputId": "a86ddfe8-fa42-4fc8-e90b-3bb6b6bd8e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1NP_K3rAGY-"
      },
      "source": [
        "### **A. MAB agent formulation :**\n",
        "\n",
        "##### The problem agent formulation involves determining the most optimal ad to display to a user at a given time instant to maximize the number of clicks on the webpage. \n",
        "\n",
        "#### The problem can be defined as :\n",
        "\n",
        " 1. There are 10 different ads to choose from, and at each time step, the MAB agent must decide which ad to display to the user. \n",
        "\n",
        " 2.  Goal is to maximize the total number of clicks obtained from the users over a specified time horizon. \n",
        "\n",
        " 3.  Each ad has an unknown click-through rate (CTR) that represents the probability of a user clicking on that ad. \n",
        " \n",
        " 4.  The MAB agent's objective is to learn the true CTR of each ad while minimizing the regret, which is the difference between the expected number of clicks obtained by displaying the best ad and the expected number of clicks obtained by displaying the chosen ad at each time step. \n",
        " \n",
        " 5.  The MAB agent must balance the exploration of less-known ads to learn their CTRs with the exploitation of the ads that are known to have higher CTRs to maximize the total number of clicks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rkwk4WIDyEL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ads_clicks = pd.read_csv('/content/drive/MyDrive/Artificial Intelligence Lab Sem - VI/Lab 6/Ads_Optimisation.csv')"
      ],
      "metadata": {
        "id": "AFc0jKhov18X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4fFqlsmE80o",
        "outputId": "b7650c43-a24d-4c5b-8bd8-25185a0ec988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ],
      "source": [
        "num_ads = len(ads_clicks.columns)\n",
        "print(num_ads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE2iRcpraSe0"
      },
      "source": [
        "### **B. Total rewards after 2000-time steps using the ε-greedy algorithm for**\n",
        "\n",
        " 1.  ε=0.01\n",
        "\n",
        "2.  ε= 0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3fVLdk5Ltvc"
      },
      "outputs": [],
      "source": [
        "# ε-greedy algorithm\n",
        "\n",
        "def epsilon_greedy(epsilon, rewards):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        # Explore: Choose a random ad\n",
        "        ad = random.randint(0, num_ads - 1)\n",
        "    else:\n",
        "        # Exploit: Choose the ad with the highest reward\n",
        "        ad = np.argmax(rewards)\n",
        "    return ad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8uusnFeLx6m"
      },
      "outputs": [],
      "source": [
        "# Initialize the rewards for each ad to 0 and create an empty list to store the rewards for each time step:\n",
        "\n",
        "rewards = [0] * num_ads\n",
        "total_rewards_01 = []\n",
        "total_rewards_03 = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6Y1Wo5xMMAe"
      },
      "outputs": [],
      "source": [
        "# Iterating the ε-greedy algorithm for 2000 time steps using ε=0.01 and ε=0.3\n",
        "\n",
        "for t in range(2000):\n",
        "\n",
        "    # Choosing ad using the epsilon-greedy algorithm with epsilon=0.01\n",
        "    ad_01 = epsilon_greedy(0.01, rewards)\n",
        "\n",
        "    # Choose the ad using the epsilon-greedy algorithm with epsilon=0.3\n",
        "    ad_03 = epsilon_greedy(0.3, rewards)\n",
        "    \n",
        "    # for epsilon = 0.01\n",
        "    # reward for the chosen ad\n",
        "    reward = ads_clicks.iloc[t][ad_01]\n",
        "    # Updating rewards for the chosen ad\n",
        "    rewards[ad_01] = rewards[ad_01] + reward\n",
        "    # Add the reward to the total rewards list for epsilon=0.01\n",
        "    total_rewards_01.append(sum(rewards))\n",
        "    \n",
        "    # for epsilon = 0.3\n",
        "    # reward for the chosen ad\n",
        "    reward = ads_clicks.iloc[t][ad_03]\n",
        "    # Updating rewards for the chosen ad\n",
        "    rewards[ad_03] = rewards[ad_03] + reward\n",
        "    # Add the reward to the total rewards list for epsilon=0.3\n",
        "    total_rewards_03.append(sum(rewards))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb06mc1mMPy_",
        "outputId": "4d337ab0-71d1-4077-c0e4-ae0c3ed737e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rewards for ε=0.01:  642\n",
            "Total rewards for ε=0.3:  642\n"
          ]
        }
      ],
      "source": [
        "print(\"Total rewards for ε=0.01: \", total_rewards_01[-1])\n",
        "print(\"Total rewards for ε=0.3: \", total_rewards_03[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mARxbQyIZ901"
      },
      "source": [
        "### **C. Compute the total rewards after 2000-time steps using the Upper-Confidence-Bound action method for c = 1.5**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Peu_lxpORHAk"
      },
      "outputs": [],
      "source": [
        "# Upper-Confidence-Bound algorithm\n",
        "\n",
        "def ucb(rewards, n, t, c=1.5):\n",
        "    # Calculate the average reward for each ad\n",
        "    average_rewards = rewards / n\n",
        "    # Calculate the upper confidence bound for each ad\n",
        "    ucb_values = average_rewards + c * np.sqrt(np.log(t + 1) / n)\n",
        "    # Choose the ad with the highest UCB value\n",
        "    ad = np.argmax(ucb_values)\n",
        "    return ad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HYRi5R9U59r"
      },
      "outputs": [],
      "source": [
        "# Initialize the rewards for each ad to 0 and create an empty list to store the rewards for each time step:\n",
        "\n",
        "rewards = np.zeros(num_ads)\n",
        "n = np.zeros(num_ads)\n",
        "total_rewards = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8ZugbLbU-tE",
        "outputId": "5efc67cf-704c-457e-9ba2-888665055c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total rewards for c=1.5:  763.0\n"
          ]
        }
      ],
      "source": [
        "# Iterating over Upper-Confidence-Bound algorithm for 2000 time steps using c=1.5:\n",
        "\n",
        "for t in range(2000):\n",
        "    # Choose the ad using the UCB algorithm\n",
        "    ad = ucb(rewards, n, t, c=1.5)\n",
        "    \n",
        "    # Get the reward for the chosen ad\n",
        "    reward = ads_clicks.iloc[t][ad]\n",
        "    # Update the rewards for the chosen ad\n",
        "    rewards[ad] = rewards[ad] + reward\n",
        "    # Update the number of times the ad has been selected\n",
        "    n[ad] = n[ad] + 1\n",
        "    # Add the reward to the total rewards list\n",
        "    total_rewards.append(sum(rewards))\n",
        "\n",
        "# Print the total rewards for c=1.5\n",
        "print(\"\\nTotal rewards for c=1.5: \", total_rewards[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **D. For all approaches, explain how the action value estimated compares to the optimal action.**"
      ],
      "metadata": {
        "id": "vkIhHMWTwv63"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSTFSOD7XvjU"
      },
      "source": [
        "> ### How the action value estimated compares to the optimal action in the ε-greedy approach :\n",
        "\n",
        "In the ε-greedy approach, action value is estimated using the **sample average method**, where the \"**estimated value of each action = average of the rewards received for that action**\". \n",
        "\n",
        "However, the sample average method can give large variance in the estimate if the number of samples is small. Thereby, the estimates may not converge to the true action values quickly. Hence, the ε-greedy approach tends to explore more at the beginning of the experiment to reduce uncertainty, and then exploit the best action based on the estimated action values later on."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### How the action value estimated compares to the optimal action in the UCB approach :\n",
        "\n",
        "The Upper-Confidence-Bound approach, on the other hand, estimates the action values using the **Upper Confidence Bound (UCB)**, which is a **combination of the average reward and an uncertainty term**. \n",
        "\n",
        "The uncertainty term ensures that actions that have not been selected many times are given a higher priority to be selected, while actions that have been selected many times are given a lower priority. This approach results in a more stable estimate of the action values, and the algorithm converges more quickly to the optimal action."
      ],
      "metadata": {
        "id": "UwGw_ovNxVCl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8-UfrIqt0Tz"
      },
      "source": [
        "In UCB, the algorithm tends to explore less often as it becomes more confident about the rewards of each arm. If the UCB algorithm becomes too confident too quickly, it may miss out on better rewards that could be obtained by exploring more.\n",
        "\n",
        "On the other hand, the epsilon-greedy algorithm always explores with a fixed probability, regardless of how confident it is about the rewards of each arm. This can lead to more exploration and potentially better rewards."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fREIH0EKzTgN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FFHy971Pwf2f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}