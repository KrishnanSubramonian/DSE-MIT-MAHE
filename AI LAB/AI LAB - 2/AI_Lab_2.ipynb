{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-agents[reverb]"
      ],
      "metadata": {
        "id": "5IpOU647vq3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f181b97-7873-41ae-eb31-0cfaff286a05"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf-agents[reverb]\n",
            "  Downloading tf_agents-0.15.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (4.4.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 KB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (1.21.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (7.1.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (2.2.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (1.14.1)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-probability>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (0.19.0)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dm-reverb~=0.10.0\n",
            "  Downloading dm_reverb-0.10.0-cp38-cp38-manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow==2.11.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents[reverb]) (2.11.0)\n",
            "Collecting rlds\n",
            "  Downloading rlds-0.1.7-py3-none-manylinux2010_x86_64.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (2.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (0.30.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (1.51.1)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (23.1.21)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (15.0.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (23.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (2.2.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (2.11.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.8/dist-packages (from dm-reverb~=0.10.0->tf-agents[reverb]) (1.3.9)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from dm-reverb~=0.10.0->tf-agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (6.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0->tf-agents[reverb]) (0.38.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (3.12.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (2.16.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (2.25.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (4.0.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0->tf-agents[reverb]) (3.2.2)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697659 sha256=f838ba1725f4ac09feb9a7c76c65791bce1d18b5d5c832f8aedcb6aa9b06edb9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/2f/ab/68bf956c5dde73c1856d981e54292cf58385fb60bca10b7acd\n",
            "Successfully built gym\n",
            "Installing collected packages: rlds, pygame, dm-reverb, gym, tf-agents\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed dm-reverb-0.10.0 gym-0.23.0 pygame-2.1.0 rlds-0.1.7 tf-agents-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import abc\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts"
      ],
      "metadata": {
        "id": "yZM3Aw4Zvr90"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "# reset() creates the initial time_step after resetting the environment.\n",
        "time_step = tf_env.reset()\n",
        "num_steps = 20\n",
        "transitions = []\n",
        "reward = 0\n",
        "for i in range(num_steps):\n",
        "  action = tf.constant([i % 2])\n",
        "  # applies the action and returns the new TimeStep.\n",
        "  next_time_step = tf_env.step(action)\n",
        "  transitions.append([time_step, action, next_time_step])\n",
        "  reward += next_time_step.reward\n",
        "  time_step = next_time_step\n",
        "\n",
        "np_transitions = tf.nest.map_structure(lambda x: x.numpy(), transitions)\n",
        "print('\\n\\n'.join(map(str, np_transitions)))\n",
        "print('Total reward:', reward.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNnIPjxFolYm",
        "outputId": "f4df5592-ed9e-4446-95ab-e86c0ba9f553"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[ 0.01530799, -0.03565129,  0.02794392, -0.02633141]],\n",
            "      dtype=float32),\n",
            " 'reward': array([0.], dtype=float32),\n",
            " 'step_type': array([0], dtype=int32)}), array([0], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[ 0.01459497, -0.23116261,  0.02741729,  0.2750355 ]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[ 0.01459497, -0.23116261,  0.02741729,  0.2750355 ]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([1], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[ 0.00997171, -0.03644235,  0.032918  , -0.00887544]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[ 0.00997171, -0.03644235,  0.032918  , -0.00887544]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([0], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[ 0.00924287, -0.23202054,  0.03274049,  0.29400906]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[ 0.00924287, -0.23202054,  0.03274049,  0.29400906]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([1], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[ 0.00460246, -0.0373803 ,  0.03862067,  0.01182916]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[ 0.00460246, -0.0373803 ,  0.03862067,  0.01182916]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([0], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[ 0.00385485, -0.23303424,  0.03885726,  0.31644288]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[ 0.00385485, -0.23303424,  0.03885726,  0.31644288]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([1], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.00080584, -0.03848668,  0.04518611,  0.03626298]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.00080584, -0.03848668,  0.04518611,  0.03626298]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([0], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.00157557, -0.23422651,  0.04591137,  0.34285322]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.00157557, -0.23422651,  0.04591137,  0.34285322]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([1], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.0062601 , -0.03978677,  0.05276844,  0.06499415]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.0062601 , -0.03978677,  0.05276844,  0.06499415]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([0], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.00705583, -0.23562402,  0.05406832,  0.37384784]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.00705583, -0.23562402,  0.05406832,  0.37384784]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([1], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.01176831, -0.04131012,  0.06154528,  0.0986913 ]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.01176831, -0.04131012,  0.06154528,  0.0986913 ]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([0], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.01259452, -0.23725767,  0.06351911,  0.41013902]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.01259452, -0.23725767,  0.06351911,  0.41013902]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([1], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.01733967, -0.04309106,  0.07172188,  0.13813852]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.01733967, -0.04309106,  0.07172188,  0.13813852]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([0], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.01820149, -0.23916307,  0.07448465,  0.45255858]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.01820149, -0.23916307,  0.07448465,  0.45255858]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([1], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.02298475, -0.04516913,  0.08353583,  0.18425274]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.02298475, -0.04516913,  0.08353583,  0.18425274]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([0], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.02388814, -0.24138083,  0.08722088,  0.50207496]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.02388814, -0.24138083,  0.08722088,  0.50207496]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([1], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.02871575, -0.04758959,  0.09726238,  0.23810352]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.02871575, -0.04758959,  0.09726238,  0.23810352]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([0], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.02966754, -0.24395677,  0.10202445,  0.55981225]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.02966754, -0.24395677,  0.10202445,  0.55981225]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([1], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.03454668, -0.05040364,  0.11322069,  0.30093545]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.03454668, -0.05040364,  0.11322069,  0.30093545]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([0], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.03555475, -0.24694197,  0.1192394 ,  0.6270712 ]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "\n",
            "[TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.03555475, -0.24694197,  0.1192394 ,  0.6270712 ]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)}), array([1], dtype=int32), TimeStep(\n",
            "{'discount': array([1.], dtype=float32),\n",
            " 'observation': array([[-0.04049359, -0.0536684 ,  0.13178083,  0.37419277]],\n",
            "      dtype=float32),\n",
            " 'reward': array([1.], dtype=float32),\n",
            " 'step_type': array([1], dtype=int32)})]\n",
            "Total reward: [20.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = suite_gym.load('CartPole-v0')\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "time_step = tf_env.reset()\n",
        "rewards = []\n",
        "steps = []\n",
        "num_episodes = 20\n",
        "\n",
        "for _ in range(num_episodes):\n",
        "  episode_reward = 0\n",
        "  episode_steps = 0\n",
        "  while not time_step.is_last():\n",
        "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
        "    time_step = tf_env.step(action)\n",
        "    episode_steps += 1\n",
        "    episode_reward += time_step.reward.numpy()\n",
        "  rewards.append(episode_reward)\n",
        "  steps.append(episode_steps)\n",
        "  time_step = tf_env.reset()\n",
        "\n",
        "num_steps = np.sum(steps)\n",
        "avg_length = np.mean(steps)\n",
        "avg_reward = np.mean(rewards)\n",
        "\n",
        "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
        "print('avg_length', avg_length, 'avg_reward:', avg_reward)"
      ],
      "metadata": {
        "id": "r3h65wXnonL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af3e221a-cbb8-41da-ec88-910fdd3433e3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_episodes: 20 num_steps: 421\n",
            "avg_length 21.05 avg_reward: 21.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Comments:**"
      ],
      "metadata": {
        "id": "i1M0NSXqmtYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rewards in both approaches can vary because of the randomness in the environment. When the environment is run for a set number of steps, the reward received will be the total reward over those steps. This gives a snapshot of the AI's performance at that time but doesn't show its overall performance over a longer period. When the environment is run for a set number of episodes, the reward received will be the average reward over all the episodes. This provides a better understanding of the AI's performance over time, considering multiple runs. However, individual runs may not perform optimally and the average may not reflect the best performance. Running the environment for a set number of episodes provides a more accurate and realistic measure of the AI's performance, as it accounts for the randomness in the environment."
      ],
      "metadata": {
        "id": "SNv6YHBzm2XE"
      }
    }
  ]
}